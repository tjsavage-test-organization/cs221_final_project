Pacman Strategy:
	We use a number of varying features for planning, and which we use qlearning to learn appropriate weights for. A good portion of our features relate to eating enemy food and protecting our own when appropriate. We have a feature tracking our movement toward enemy food, as well as the percentage of our food left and the percentage of the enemey food left.  We have features for the distance to the closest enemy food squared, and well as to the closes food of ours squared.
	We also track a number of basic descriptive features.  We track of "degrees of freedom" of movement, that is, how many legal moves are from a position, and try to keep it high, staying out of tunnels and dead ends.  We track the average distance to all our allies, the closest enemy as a ghost, the closest enemy as a pacman, and the square of these values.
	We also have features for a notion of "territories" - that is, the number of foods that a given agent is closer to than any other agent.  Agents attempt to maximize the territory they control, in order to effectively split up the playing field.

We use a couple of different AI techniques to plan and learn.  To calculate weights for our various features in order to make decisions, we use qlearning. For each weight after each action, we calculate a correction where correct = reward + (discount * next_state_value) - evaluate(state, action). We then add the correction value multiplied by our learning alpha to the original weight. Our reward funciton includes rewards for eating food and capsules, and negatively rewards getting eaten as a pacman. 
